{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi98Zo-mbhf9",
        "outputId": "a5163ef7-a85d-48e6-9ecf-375acc276b3f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lx8nLVxPTg3",
        "outputId": "de6821cc-da50-4d7b-c66a-7bd16085eb45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to results.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import csv\n",
        "import re\n",
        "import argparse\n",
        "from typing import List, Dict, Optional\n",
        "import sys\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize LLM for entity recognition using a fully public NER model\n",
        "llm = pipeline(\"ner\", model=\"nlpaueb/legal-bert-base-uncased\", aggregation_strategy=\"simple\")  # Publicly available NER model\n",
        "\n",
        "# Constants\n",
        "PUBMED_API_URL = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "DETAILS_API_URL = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
        "\n",
        "def fetch_pubmed_ids(query: str) -> List[str]:\n",
        "    \"\"\"Fetch PubMed IDs based on a search query.\"\"\"\n",
        "    params = {\n",
        "        \"db\": \"pubmed\",\n",
        "        \"term\": query,\n",
        "        \"retmode\": \"json\",\n",
        "        \"retmax\": 10  # Adjust as needed\n",
        "    }\n",
        "    response = requests.get(PUBMED_API_URL, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return data.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
        "    return []\n",
        "\n",
        "def fetch_paper_details(pubmed_ids: List[str]) -> List[Dict[str, str]]:\n",
        "    \"\"\"Fetch details for given PubMed IDs.\"\"\"\n",
        "    params = {\n",
        "        \"db\": \"pubmed\",\n",
        "        \"id\": \",\".join(pubmed_ids),\n",
        "        \"retmode\": \"json\"\n",
        "    }\n",
        "    response = requests.get(DETAILS_API_URL, params=params)\n",
        "    papers = []\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        for paper_id in pubmed_ids:\n",
        "            paper_info = data.get(\"result\", {}).get(paper_id, {})\n",
        "            papers.append({\n",
        "                \"PubmedID\": paper_id,\n",
        "                \"Title\": paper_info.get(\"title\", \"Unknown\"),\n",
        "                \"Publication Date\": paper_info.get(\"pubdate\", \"Unknown\"),\n",
        "                \"Non-academic Author(s)\": extract_non_academic_authors(paper_info.get(\"authors\", [])),\n",
        "                \"Company Affiliation(s)\": extract_company_affiliations_llm(paper_info.get(\"source\", \"\")),\n",
        "                \"Corresponding Author Email\": extract_email(paper_info.get(\"source\", \"\"))\n",
        "            })\n",
        "    return papers\n",
        "\n",
        "def extract_non_academic_authors(authors: List[Dict[str, str]]) -> str:\n",
        "    \"\"\"Identify non-academic authors based on heuristics.\"\"\"\n",
        "    non_academic_authors = [author[\"name\"] for author in authors if not re.search(r\"university|lab|institute\", author.get(\"affiliation\", \"\"), re.IGNORECASE)]\n",
        "    return \", \".join(non_academic_authors)\n",
        "\n",
        "def extract_company_affiliations_llm(source: str) -> str:\n",
        "    \"\"\"Use an LLM to extract company affiliations from text.\"\"\"\n",
        "    entities = [entity['word'] for entity in llm(source) if entity['entity_group'] == 'ORG' and entity['score'] > 0.8]  # Filtering based on confidence score\n",
        "    return \", \".join(set(entities))\n",
        "\n",
        "def extract_email(source: str) -> str:\n",
        "    \"\"\"Extract email address from source data.\"\"\"\n",
        "    email_match = re.search(r\"[\\w.-]+@[\\w.-]+\\.[a-z]{2,}\", source)\n",
        "    return email_match.group(0) if email_match else \"\"\n",
        "\n",
        "def save_to_csv(papers: List[Dict[str, str]], filename: str):\n",
        "    \"\"\"Save research papers to a CSV file.\"\"\"\n",
        "    keys = [\"PubmedID\", \"Title\", \"Publication Date\", \"Non-academic Author(s)\", \"Company Affiliation(s)\", \"Corresponding Author Email\"]\n",
        "    with open(filename, \"w\", newline=\"\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(papers)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Fetch research papers from PubMed.\")\n",
        "    parser.add_argument(\"query\", type=str, help=\"Search query for PubMed.\")\n",
        "    parser.add_argument(\"-f\", \"--file\", type=str, help=\"Filename to save results as CSV.\")\n",
        "    parser.add_argument(\"-d\", \"--debug\", action=\"store_true\", help=\"Enable debug mode.\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    pubmed_ids = fetch_pubmed_ids(args.query)\n",
        "    if args.debug:\n",
        "        print(\"Fetched PubMed IDs:\", pubmed_ids)\n",
        "\n",
        "    papers = fetch_paper_details(pubmed_ids)\n",
        "    if args.file:\n",
        "        save_to_csv(papers, args.file)\n",
        "        print(f\"Results saved to {args.file}\")\n",
        "    else:\n",
        "        for paper in papers:\n",
        "            print(paper)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if \"get_ipython\" in globals():  # Detect if running in Jupyter/Colab\n",
        "        sys.argv = [\"fetch_pubmed_papers.py\", \"cancer research\", \"-f\", \"results.csv\"]\n",
        "    main()"
      ]
    }
  ]
}